# As in Eijkelboom et al. (2024), taken from: https://github.com/cvignac/DiGress
# Number of layers in Graph Transformer
n_layers: 5
# The number of classes and hidden dimension of the model: 
num_classes: {'X': 4, 'E': 5, 'y': 1}
hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 128}
hidden_dims : {'dx': 256, 'de': 64, 'dy': 64, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 128}
# Training parameters
n_epochs: 1000
n_epochs_tuning: 5
batch_size: 256 # originally, 512
scheduler: cosine_annealing
lr: 0.001 #0.0002
ema_decay: 0.999
weight_decay: 1e-12
optimizer: adamw
seed: 0
# Inference parameters
# Number of nodes for the generated graphs
n_nodes: 3
# Weights for the loss function
lambda_train: [5, 0]
n_node_classes: 4
n_edge_classes: 5
alpha_max: 8
num_integration_steps: 100
n_iter: 3